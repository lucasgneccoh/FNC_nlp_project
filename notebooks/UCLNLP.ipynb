{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UCLNLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KriImKeF2bC_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AokobFLV1vKW",
        "outputId": "1aba32ff-9e29-4b3e-a933-15bc5a7a8732",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/lucasgneccoh/FNC_nlp_project.git\n",
        "\n",
        "os.chdir(\"/content/FNC_nlp_project\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'FNC_nlp_project'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
            "remote: Compressing objects: 100% (136/136), done.\u001b[K\n",
            "remote: Total 144 (delta 63), reused 29 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (144/144), 26.28 MiB | 27.69 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWdvcT5Y1vFh"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NpLAbjL7bp3"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTU8mcEZ76UR"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9tPzkim1u8z"
      },
      "source": [
        "train_bodies = pd.read_csv('data/FNC_data/train_bodies.csv') # BodyID, Body\n",
        "train_stances = pd.read_csv('data/FNC_data/train_stances.csv') # Headline, BodyID, Stance\n",
        "\n",
        "test_bodies = pd.read_csv('data/FNC_data/competition_test_bodies.csv') # BodyID, Body\n",
        "test_stances = pd.read_csv('data/FNC_data/competition_test_stances.csv') # Headline, BodyID, Stance"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stU9VDSkNJZ6"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "we want to create our vectorised representations of the training and test sets\n",
        "\n",
        "So first we set the stop words, normalising the tables,then calculating the tf and tfidf representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vcnhdv50kRu"
      },
      "source": [
        "# data preprocessing parameters\n",
        "lim_unigram = 5000\n",
        "stop_words = [\n",
        "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
        "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
        "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
        "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
        "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
        "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
        "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
        "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
        "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
        "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
        "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
        "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
        "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
        "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
        "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
        "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
        "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
        "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
        "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
        "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
        "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
        "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
        "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
        "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
        "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
        "        ]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm-RpF8Y8RK8"
      },
      "source": [
        "We process the data in normalized tables (pandas Dataframes and Series)  \n",
        "In the end we have 3 objects for the train set and 3 for the test set  \n",
        "if X is 'train' or 'test', we have:  \n",
        "- X_headlines: Series with the headlines   \n",
        "- X_bodies: Series with the bodies\n",
        "- X_stances: relation table with the X_healine and X_bodies indexes ('head id' and 'body id') and the labels ('stance')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYRavp0FA24M"
      },
      "source": [
        "cat = pd.Categorical(train_stances['Headline'])\n",
        "train_headlines = pd.Series(cat.categories,name='headline')\n",
        "train_stances['head id'] = cat.codes\n",
        "train_stances = train_stances.drop(columns='Headline')\n",
        "\n",
        "cat = pd.Categorical(train_stances['Body ID'])\n",
        "train_bodies = pd.Series([train_bodies['articleBody'][(train_bodies['Body ID'] == i)].values[0] for i in cat.categories]\n",
        "    ,name='article body')\n",
        "train_stances['body id'] = cat.codes\n",
        "train_stances = train_stances.drop(columns='Body ID')\n",
        "\n",
        "cat = pd.Categorical(train_stances['Stance'])\n",
        "stances = pd.Series(cat.categories,name='stance')\n",
        "train_stances['stance'] = cat.codes\n",
        "train_stances = train_stances.drop(columns='Stance')\n",
        "\n",
        "cat = pd.Categorical(test_stances['Headline'])\n",
        "test_headlines = pd.Series(cat.categories,name='headline')\n",
        "test_stances['head id'] = cat.codes\n",
        "test_stances = test_stances.drop(columns='Headline')\n",
        "\n",
        "cat = pd.Categorical(test_stances['Body ID'])\n",
        "test_bodies = pd.Series([test_bodies['articleBody'][(test_bodies['Body ID'] == i)].values[0] for i in cat.categories]\n",
        "    ,name='article body')\n",
        "test_stances['body id'] = cat.codes\n",
        "test_stances = test_stances.drop(columns='Body ID')\n",
        "\n",
        "cat = pd.Categorical(test_stances['Stance'], categories = stances)\n",
        "test_stances['stance'] = cat.codes\n",
        "test_stances = test_stances.drop(columns='Stance')\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHh4tNuE86Ek"
      },
      "source": [
        "Now we create vectorizers and BOW and TF arrays for train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crb89GnG9CY4"
      },
      "source": [
        "bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
        "bow = bow_vectorizer.fit_transform(list(train_headlines) + list(train_bodies))  # Train set only\n",
        "\n",
        "tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
        "tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
        "    fit(list(train_headlines) + list(train_bodies) + \\\n",
        "        list(test_headlines)  + list(test_bodies)\n",
        "        )  # Train and test sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqAwS3Cf9FAF"
      },
      "source": [
        "We encode the headlines and bodies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFcwFb_vqMLB"
      },
      "source": [
        "bow = bow_vectorizer.transform(list(train_headlines)).toarray()\n",
        "head_vect_train = tfreq_vectorizer.transform(bow).toarray()\n",
        "bow = bow_vectorizer.transform(list(test_headlines)).toarray()\n",
        "head_vect_test = tfreq_vectorizer.transform(bow).toarray()\n",
        "bow =  bow_vectorizer.transform(list(train_bodies)).toarray()\n",
        "body_vect_train = tfreq_vectorizer.transform(bow).toarray()\n",
        "bow =  bow_vectorizer.transform(list(test_bodies)).toarray()\n",
        "body_vect_test = tfreq_vectorizer.transform(bow).toarray()\n",
        "\n",
        "head_tfidf_train = tfidf_vectorizer.transform(list(train_headlines)).toarray()\n",
        "head_tfidf_test = tfidf_vectorizer.transform(list(test_headlines)).toarray()\n",
        "body_tfidf_train = tfidf_vectorizer.transform(list(train_bodies)).toarray()\n",
        "body_tfidf_test = tfidf_vectorizer.transform(list(test_bodies)).toarray()\n",
        "cos_sim = lambda i : cosine_similarity(\n",
        "    head_tfidf_train[train_stances['head id'][i]].reshape(1,-1),\n",
        "    body_tfidf_train[train_stances['body id'][i]].reshape(1,-1)\n",
        "    ).squeeze()\n",
        "tfidf_cos_train = np.array([cos_sim(i) for i in range(len(train_stances))])\n",
        "cos_sim = lambda i : cosine_similarity(\n",
        "    head_tfidf_test[test_stances['head id'][i]].reshape(1,-1),\n",
        "    body_tfidf_test[test_stances['body id'][i]].reshape(1,-1)\n",
        "    ).squeeze()\n",
        "tfidf_cos_test = np.array([cos_sim(i) for i in range(len(test_stances))])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5d-thmB9TyW"
      },
      "source": [
        "Finally we use our vector representations to compute the train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edkyOL6exs4v"
      },
      "source": [
        "train_set = np.c_[head_vect_train[train_stances['head id']],\n",
        "                  body_vect_train[train_stances['body id']],\n",
        "                  tfidf_cos_train\n",
        "                  ]\n",
        "test_set = np.c_[head_vect_test[test_stances['head id']],\n",
        "                 body_vect_test[test_stances['body id']],\n",
        "                 tfidf_cos_test\n",
        "                 ]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGOD7jW18Juc"
      },
      "source": [
        "## memory check and clean\n",
        "after this preprocessing, we can discard some heavy variables to free some memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzKhBaLg7YyC"
      },
      "source": [
        "del tfreq, body_vect_train, body_tfidf_train, \\\n",
        "    head_tfidf_train, head_vect_train, bow, \\\n",
        "    body_vect_test, body_tfidf_test, head_vect_test, \\\n",
        "    head_tfidf_test\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfQx7la-90Fz"
      },
      "source": [
        "and check if what is left in memory is pertinent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpSjm1De7nqF",
        "outputId": "3a0a0b81-694e-4c05-a945-f76e7a220714"
      },
      "source": [
        "import sys\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "                         key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     train_set:  3.7 GiB\n",
            "                      test_set:  1.9 GiB\n",
            "                  train_bodies:  6.2 MiB\n",
            "                   test_bodies:  3.4 MiB\n",
            "               tfidf_cos_train: 390.5 KiB\n",
            "                 train_stances: 244.2 KiB\n",
            "               train_headlines: 243.1 KiB\n",
            "                tfidf_cos_test: 198.6 KiB\n",
            "                test_headlines: 135.9 KiB\n",
            "                  test_stances: 124.2 KiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra1SSTLjNzoU"
      },
      "source": [
        "# model, training and results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THWrvt1n_HYB"
      },
      "source": [
        "## model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAYjrn2e-FCh"
      },
      "source": [
        "we set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzJtZGpO-Eyh"
      },
      "source": [
        "display_summary = True\n",
        "target_size = 4 # 4 different classes, (stances)\n",
        "hidden_size = 100 # hidden layer width\n",
        "train_keep_prob = 0.6 # for dropouts\n",
        "l2_alpha = 0.00001 # regularization parameter\n",
        "learn_rate = 0.01 # learning rate\n",
        "clip_ratio = 5 # clip ratio for the gradient norm clipping\n",
        "batch_size_train = 512 # pretty self-explanatory\n",
        "epochs = 90 # number of epochs for training"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrgcvb2N-D-z"
      },
      "source": [
        "We define the model with just one hiden layer as in the UCLNLP solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipyKu82uxsgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b92cc65-9354-4e80-e4db-38bda27693fa"
      },
      "source": [
        "feature_size = train_set.shape[1]\n",
        "regul = keras.regularizers.l2(l2_alpha)\n",
        "\n",
        "input = keras.Input(shape = (feature_size,), name='features')\n",
        "hidden = keras.layers.Dense(hidden_size, activation='relu',kernel_regularizer=regul)(input)\n",
        "hidden = keras.layers.Dropout(1-train_keep_prob, name = 'hidden')(hidden)\n",
        "output = keras.layers.Dense(target_size,kernel_regularizer=regul)(hidden)\n",
        "output = keras.layers.Dropout(1-train_keep_prob, name = 'output')(output)\n",
        "softmax = keras.layers.Activation('softmax', name='classifier')(output)\n",
        "\n",
        "model = keras.Model(inputs=input, outputs=softmax)\n",
        "\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits='True')\n",
        "model.compile(optimizer = keras.optimizers.Adam(learning_rate=learn_rate, clipnorm=clip_ratio),\n",
        "            loss=loss)\n",
        "\n",
        "if display_summary : model.summary ()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "features (InputLayer)        [(None, 10001)]           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               1000200   \n",
            "_________________________________________________________________\n",
            "hidden (Dropout)             (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 404       \n",
            "_________________________________________________________________\n",
            "output (Dropout)             (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "classifier (Activation)      (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 1,000,604\n",
            "Trainable params: 1,000,604\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iGZs312N6Ap"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udErl-i-Ywae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1131532e-56f4-4dac-bf90-8818185d4d59"
      },
      "source": [
        "model.fit(train_set, train_stances['stance'].to_numpy(),batch_size_train,epochs,validation_data=(test_set, test_stances['stance'].to_numpy()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/90\n",
            "98/98 [==============================] - 10s 94ms/step - loss: 0.8488 - val_loss: 0.4670\n",
            "Epoch 2/90\n",
            "98/98 [==============================] - 8s 77ms/step - loss: 0.4855 - val_loss: 0.4225\n",
            "Epoch 3/90\n",
            "98/98 [==============================] - 8s 77ms/step - loss: 0.4405 - val_loss: 0.4218\n",
            "Epoch 4/90\n",
            "98/98 [==============================] - 8s 77ms/step - loss: 0.4247 - val_loss: 0.4219\n",
            "Epoch 5/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4153 - val_loss: 0.4447\n",
            "Epoch 6/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4197 - val_loss: 0.4340\n",
            "Epoch 7/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4141 - val_loss: 0.4368\n",
            "Epoch 8/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4061 - val_loss: 0.4532\n",
            "Epoch 9/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4036 - val_loss: 0.4380\n",
            "Epoch 10/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4110 - val_loss: 0.4670\n",
            "Epoch 11/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4043 - val_loss: 0.4664\n",
            "Epoch 12/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4032 - val_loss: 0.4684\n",
            "Epoch 13/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4089 - val_loss: 0.4658\n",
            "Epoch 14/90\n",
            "98/98 [==============================] - 8s 77ms/step - loss: 0.4052 - val_loss: 0.4738\n",
            "Epoch 15/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.3999 - val_loss: 0.4657\n",
            "Epoch 16/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4036 - val_loss: 0.4630\n",
            "Epoch 17/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4046 - val_loss: 0.4865\n",
            "Epoch 18/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4155 - val_loss: 0.4509\n",
            "Epoch 19/90\n",
            "98/98 [==============================] - 8s 77ms/step - loss: 0.4159 - val_loss: 0.4531\n",
            "Epoch 20/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4039 - val_loss: 0.4619\n",
            "Epoch 21/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4106 - val_loss: 0.4475\n",
            "Epoch 22/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4101 - val_loss: 0.4695\n",
            "Epoch 23/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4096 - val_loss: 0.4609\n",
            "Epoch 24/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4094 - val_loss: 0.4684\n",
            "Epoch 25/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4072 - val_loss: 0.4665\n",
            "Epoch 26/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4017 - val_loss: 0.4808\n",
            "Epoch 27/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4046 - val_loss: 0.4863\n",
            "Epoch 28/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.3996 - val_loss: 0.4874\n",
            "Epoch 29/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4095 - val_loss: 0.4627\n",
            "Epoch 30/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4013 - val_loss: 0.4937\n",
            "Epoch 31/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4112 - val_loss: 0.4679\n",
            "Epoch 32/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4083 - val_loss: 0.4708\n",
            "Epoch 33/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4044 - val_loss: 0.4679\n",
            "Epoch 34/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4045 - val_loss: 0.4598\n",
            "Epoch 35/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4037 - val_loss: 0.4917\n",
            "Epoch 36/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4056 - val_loss: 0.4758\n",
            "Epoch 37/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4058 - val_loss: 0.4854\n",
            "Epoch 38/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4125 - val_loss: 0.4876\n",
            "Epoch 39/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4083 - val_loss: 0.4935\n",
            "Epoch 40/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4066 - val_loss: 0.4808\n",
            "Epoch 41/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4034 - val_loss: 0.4775\n",
            "Epoch 42/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4116 - val_loss: 0.4900\n",
            "Epoch 43/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4062 - val_loss: 0.4911\n",
            "Epoch 44/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.3996 - val_loss: 0.4915\n",
            "Epoch 45/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3996 - val_loss: 0.4801\n",
            "Epoch 46/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4018 - val_loss: 0.4671\n",
            "Epoch 47/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4065 - val_loss: 0.4854\n",
            "Epoch 48/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4025 - val_loss: 0.4699\n",
            "Epoch 49/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4003 - val_loss: 0.4975\n",
            "Epoch 50/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4056 - val_loss: 0.4856\n",
            "Epoch 51/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.3997 - val_loss: 0.4835\n",
            "Epoch 52/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4006 - val_loss: 0.4916\n",
            "Epoch 53/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4037 - val_loss: 0.4906\n",
            "Epoch 54/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.3996 - val_loss: 0.4964\n",
            "Epoch 55/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4001 - val_loss: 0.4781\n",
            "Epoch 56/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4046 - val_loss: 0.4911\n",
            "Epoch 57/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4122 - val_loss: 0.4592\n",
            "Epoch 58/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4088 - val_loss: 0.4696\n",
            "Epoch 59/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4095 - val_loss: 0.4730\n",
            "Epoch 60/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4029 - val_loss: 0.4737\n",
            "Epoch 61/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3973 - val_loss: 0.4810\n",
            "Epoch 62/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3935 - val_loss: 0.4754\n",
            "Epoch 63/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3971 - val_loss: 0.4695\n",
            "Epoch 64/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4016 - val_loss: 0.4743\n",
            "Epoch 65/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4017 - val_loss: 0.4705\n",
            "Epoch 66/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4003 - val_loss: 0.4641\n",
            "Epoch 67/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4007 - val_loss: 0.4489\n",
            "Epoch 68/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3965 - val_loss: 0.4570\n",
            "Epoch 69/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3963 - val_loss: 0.4554\n",
            "Epoch 70/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4026 - val_loss: 0.4545\n",
            "Epoch 71/90\n",
            "98/98 [==============================] - 9s 89ms/step - loss: 0.4072 - val_loss: 0.4747\n",
            "Epoch 72/90\n",
            "98/98 [==============================] - 8s 80ms/step - loss: 0.4055 - val_loss: 0.4741\n",
            "Epoch 73/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4077 - val_loss: 0.5096\n",
            "Epoch 74/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4046 - val_loss: 0.4759\n",
            "Epoch 75/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4087 - val_loss: 0.4751\n",
            "Epoch 76/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4030 - val_loss: 0.4795\n",
            "Epoch 77/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.3990 - val_loss: 0.4689\n",
            "Epoch 78/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.4038 - val_loss: 0.4798\n",
            "Epoch 79/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4015 - val_loss: 0.4705\n",
            "Epoch 80/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3987 - val_loss: 0.4723\n",
            "Epoch 81/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3987 - val_loss: 0.4793\n",
            "Epoch 82/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4071 - val_loss: 0.4926\n",
            "Epoch 83/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.3970 - val_loss: 0.4646\n",
            "Epoch 84/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3942 - val_loss: 0.5021\n",
            "Epoch 85/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3986 - val_loss: 0.4683\n",
            "Epoch 86/90\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.3971 - val_loss: 0.4730\n",
            "Epoch 87/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3959 - val_loss: 0.4751\n",
            "Epoch 88/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.4027 - val_loss: 0.4875\n",
            "Epoch 89/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3978 - val_loss: 0.4719\n",
            "Epoch 90/90\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3991 - val_loss: 0.4631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd434a971d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bShkTBR9_Koh"
      },
      "source": [
        "## results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LN1xwm5_NBN"
      },
      "source": [
        "We use the metric given for the challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBpHke2ueLtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273d3159-2aef-4b99-c3fc-6f84f8d74358"
      },
      "source": [
        "def metric (X,Y) :\n",
        "    lvl1 = 0.25 * ((X == 3 ) == (Y == 3)).sum()\n",
        "    lvl2 = 0.75 * ((X < 3) & (X == Y)).sum()\n",
        "    return lvl1 + lvl2\n",
        "\n",
        "res = metric(model.predict(test_set).argmax(1), test_stances['stance'])\n",
        "print(f'for this model the score is {res}, the baseline score was 9521.5')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for this model the score is 9582.0, the baseline score was 9521.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}