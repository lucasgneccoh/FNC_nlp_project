{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Plx0htsMwIya",
        "GsFPIdUpwF2U",
        "JPbDqvir6lS8"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasgneccoh/FNC_nlp_project/blob/main/notebooks/LSTM_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "csVOtXJkQgSV"
      },
      "source": [
        "#@title Notebook variables { run: \"auto\" }\n",
        "\n",
        "#@markdown These variables will define important behaviour when executing the notebook. Run this cell once, and then changing the parameters will re-run it automatically\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Data section\n",
        "#@markdown **merge**: Tells whether to merge train and test set of the FNC data, shuffle and re-split into test and train set.\n",
        "merge = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Embedding layer section\n",
        "#@markdown **model_source**: You can choose to load pre-trained word embeddings from `torchtext` or `gensim`\n",
        "model_source = \"torchtext\" #@param [\"torchtext\", \"gensim\"]\n",
        "\n",
        "#@markdown **pretrained_model**: Depending on the *model_source*, choose a pretrained model. See [here](https://torchtext.readthedocs.io/en/latest/vocab.html?highlight=glove#pretrained-aliases) for the `torchtext` available models, and [here](https://github.com/RaRe-Technologies/gensim-data#models) for the gensim models.\n",
        "\n",
        "#@markdown ***Note***: For gensim, we dont use the gensim download api as we had problems with it. We directly download the models and load them using gensim, so if you want to use another model, use `wget` or any download mean to download it on the Colab machine (or yours) and then load the KeyedVectors. See section *Embedding layer* for examples \n",
        "\n",
        "#@markdown Google News model was taken from [here](https://code.google.com/archive/p/word2vec/). Gensim fast text is downloaded from [https://fasttext.cc/docs/en/english-vectors.html]. The links to these and more models are available at the gensim githuub repository\n",
        "pretrained_model = \"charngram.100d\" #@param [\"charngram.100d\",\"fasttext.en.300d\",\"fasttext.simple.300d\",\"glove.6B.200d\",\"glove.6B.300d\",\"glove.840B.300d\", \"gensim_google_news\", \"gensim_fastText\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2xH5XPrUfl2"
      },
      "source": [
        "# Imports and initial definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZi2Fh26An6g"
      },
      "source": [
        "## Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaeCOJVfAm-r",
        "outputId": "dcb943fa-0400-4fbd-e151-788671d6a9f0"
      },
      "source": [
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/lucasgneccoh/FNC_nlp_project.git\n",
        "\n",
        "root_dir = \"/content/FNC_nlp_project\"\n",
        "os.chdir(root_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'FNC_nlp_project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsVsDikDUjjY"
      },
      "source": [
        "## Install gensim and make imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9bAOeqzBz1B",
        "outputId": "a57c98af-3ba4-4ef8-f32a-d15f11daa02f"
      },
      "source": [
        "!pip install gensim==4.0.0b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim==4.0.0b\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/47/16e2e4f34ec7534db21facf505c5a17e3ba10cbce72f675721277628d454/gensim-4.0.0b0-cp37-cp37m-manylinux1_x86_64.whl (24.0MB)\n",
            "\u001b[K     |████████████████████████████████| 24.0MB 132kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0b) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0b) (5.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0b) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.0b0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sYqX3jOALkD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gensim\n",
        "import torch\n",
        "import torchtext\n",
        "import spacy\n",
        "from nltk import tokenize\n",
        "import pandas as pd\n",
        "import gensim.parsing.preprocessing as prep\n",
        "import re\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Example, Iterator, Dataset\n",
        "import torchtext.vocab as vocab\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5CceyIeUWBx",
        "outputId": "13e67449-6454-4072-a178-194d11836fbb"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqNCsjCtLlC0"
      },
      "source": [
        "# Data\n",
        "Here we read the data and process it to create the needed datasets and dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoyOr3BYcGBO"
      },
      "source": [
        "## Read and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9PL_8mPZk-3"
      },
      "source": [
        "def process_data_frames(stances, bodies):\n",
        "    \"\"\"\n",
        "    Create three pandas DataFrame objects representing the FNC data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stances: pandas.DataFrame\n",
        "        DataFrame with the following structure: Headline, Body ID, Stance\n",
        "        Stance takes the values 'unrelated', 'agree', 'disagree', 'discuss'\n",
        "\n",
        "    bodies: pandas.DataFrame\n",
        "        DataFrame with the following structure: Body ID, Body\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        stances: Body ID, Headline ID, stance label\n",
        "        bodies: Body ID, Body\n",
        "        Headlines: Headline ID, Headline\n",
        "    \"\"\"\n",
        "    uni = stances['Headline'].unique()\n",
        "    uni_keys = dict(zip(uni, range(len(uni))))\n",
        "    labels_dict = {'unrelated':0, 'agree':1, 'disagree':2, 'discuss':3}\n",
        "    stances['Headline ID'] = stances['Headline'].map(uni_keys)\n",
        "    stances['Stance'] = stances['Stance'].map(labels_dict)\n",
        "    headlines = stances.loc[:,['Headline ID', 'Headline']].drop_duplicates(subset=['Headline ID']).set_index('Headline ID')\n",
        "\n",
        "    bodies.set_index('Body ID', inplace=True)\n",
        "    bodies.rename(columns={'articleBody': 'Body'}, inplace=True)\n",
        "    stances.drop(columns=['Headline'], inplace=True)\n",
        "\n",
        "    return stances, bodies, headlines\n",
        "\n",
        "class FNC_Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\" Dataset with the FNC data (headline, body, stance)\n",
        "        We represent each observation using a dictionary.\n",
        "        The text can be preprocessed with a custom function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        stances_file: str\n",
        "            Name of the original stances csv file containing Headline, Body ID\n",
        "            and Stance\n",
        "\n",
        "        bodies_file: str\n",
        "            Name of the original bodies csv file containing Body ID and Body\n",
        "\n",
        "        path: str\n",
        "            Base path to prepend to the the file names. Absolute path to the \n",
        "            csv files will be the one returned by os.path.join(`path`, `file_name`)\n",
        "\n",
        "        processing: function\n",
        "            Function to process a sentence. It takes a str and returns a list of\n",
        "            str representing the tokenized and preprocessed sentenced \n",
        "    \"\"\"\n",
        "  \n",
        "    def __init__(self, stances_file, bodies_file, path, processing):\n",
        "        \"\"\" Creates an instance of the FNC_Dataset by loading the original csv\n",
        "            files and creating internal pandas DataFrame object with the data\n",
        "        \"\"\"\n",
        "        stances = pd.read_csv(os.path.join(path, stances_file))\n",
        "        bodies = pd.read_csv(os.path.join(path, bodies_file))\n",
        "        self.stances, self.bodies, self.headlines = process_data_frames(stances, bodies)\n",
        "        # Function to pass from line of text to input\n",
        "        self.processing = processing\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.stances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        row = self.stances.iloc[idx]    \n",
        "        h, b = self.headlines.loc[row.at['Headline ID']].get('Headline'), self.bodies.loc[row.at['Body ID']].get('Body')\n",
        "        h, b = self.processing(h), self.processing(b) \n",
        "        s = row.at['Stance']\n",
        "        return {'headline':h, 'body':b, 'stance':s}\n",
        "        \n",
        "\n",
        "\"\"\" \n",
        "We are going to use torchtext in the following cells.\n",
        "torchtext adds the processing on the Field class directly and\n",
        "not in the \"source\" dataset, so for now we do not preprocess\n",
        "\"\"\"\n",
        "processing = lambda x: x \n",
        "\n",
        "path = 'data/FNC_data'\n",
        "stances_file = 'train_stances.csv'\n",
        "bodies_file = 'train_bodies.csv'\n",
        "\n",
        "data_FNC = FNC_Dataset(stances_file, bodies_file, path, processing)\n",
        "\n",
        "\n",
        "path = 'data/FNC_data'\n",
        "stances_file = 'competition_test_stances.csv'\n",
        "bodies_file = 'competition_test_bodies.csv'\n",
        "\n",
        "test_FNC = FNC_Dataset(stances_file, bodies_file, path, processing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwyttm8PysKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e004ce-8865-459f-f0ac-c1b901992f86"
      },
      "source": [
        "# Example of data sample with current configuration\n",
        "test_FNC[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'body': 'When faced with the choice of feasting on a fine meal of human while listening to Justin Bieber\\'s music or fleeing to blessed — but hungry — quiet, one bear in Russia decided that silence is indeed golden.\\n\\nA report in the Daily Mail details Russian fisherman Igor Vorozhbitsyn\\'s unfortunate encounter with the brown bear, who attacked him from behind as he was walking to his favorite fishing spot in the Yakutia Republic. But as the bear was beginning to inflict serious injury on Vorozhbitsyn, his cellphone rang, and the ringtone of Justin Bieber hit \"Baby\" startled the animal, causing it to beat a hasty retreat.\\n\\n\"I know that sort of ringtone isn\\'t to everyone\\'s taste but my granddaughter loaded it onto my phone for a joke,\" Vorozhbitsyn said. Nice work, kid — you just saved Grandpa\\'s life with nothing more than a tween anthem.\\n\\n- - Mike Barry',\n",
              " 'headline': \"Next-generation Apple iPhones' features leaked\",\n",
              " 'stance': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgIhACX0Mg0s"
      },
      "source": [
        "## Select preprocessing\n",
        "Here we show two possible processing functions. We chose the composition of functions provided by `gensim` because of flexibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjvvBaU-J1_t",
        "outputId": "74b8df61-7279-4c04-ae4e-bc273d4fa313"
      },
      "source": [
        "\"\"\" Compare tokenizers    \n",
        "\"\"\"\n",
        "spacy_en = spacy.load('en')\n",
        "def spacy_tokenizer(text): # create a tokenizer function\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "# Use the one used before\n",
        "CUSTOM_FILTERS = [lambda x: x.lower(), lambda x: re.sub('\\W+',' ',x) ,\\\n",
        "                           prep.strip_tags, prep.strip_punctuation, \\\n",
        "                           prep.strip_punctuation2, prep.strip_multiple_whitespaces, \\\n",
        "                           prep.strip_numeric, prep.remove_stopwords, prep.strip_short]\n",
        "processing = lambda x: prep.preprocess_string(x, CUSTOM_FILTERS)\n",
        "\n",
        "\n",
        "\n",
        "print(data_FNC[0]['headline'])\n",
        "print('spacey')\n",
        "print(spacy_tokenizer(data_FNC[0]['headline']))\n",
        "\n",
        "print('gensim')\n",
        "print(processing(data_FNC[0]['headline']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Police find mass graves with at least '15 bodies' near Mexico town where 43 students disappeared after police clash\n",
            "spacey\n",
            "['Police', 'find', 'mass', 'graves', 'with', 'at', 'least', \"'\", '15', 'bodies', \"'\", 'near', 'Mexico', 'town', 'where', '43', 'students', 'disappeared', 'after', 'police', 'clash']\n",
            "gensim\n",
            "['police', 'mass', 'graves', 'bodies', 'near', 'mexico', 'town', 'students', 'disappeared', 'police', 'clash']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AzOUgNnMp8k"
      },
      "source": [
        "## Merge and shuffle train and test data\n",
        "\n",
        "\n",
        "To choose whether to merge and re-split or not, define the `merge` variable on the **Variable definition cell** on the top of the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxhzwKBxXYDI"
      },
      "source": [
        "\"\"\"\n",
        "We are considering to merge the train and test set\n",
        "and then resample the test set, see if we get more \n",
        "normal results\n",
        "\n",
        "merge is defined on the Variable definition cell\n",
        "\"\"\"\n",
        "\n",
        "if merge:\n",
        "    all_FNC = torch.utils.data.ConcatDataset([data_FNC, test_FNC])\n",
        "    new_data_FNC, new_test_FNC = torch.utils.data.random_split(all_FNC, [len(data_FNC), len(test_FNC)], generator=torch.Generator().manual_seed(42))\n",
        "else:\n",
        "    new_data_FNC, new_test_FNC = data_FNC, test_FNC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaTTpJTkabym",
        "outputId": "9e040d58-62fb-4926-df3d-ebdd254aa63b"
      },
      "source": [
        "print(f\"data_FNC: {len(data_FNC)}, {len(new_data_FNC)}\")\n",
        "print(f\"test_FNC: {len(test_FNC)}, {len(new_test_FNC)}\")\n",
        "N = len(test_FNC)\n",
        "i = np.random.randint(N)\n",
        "print(test_FNC[i])\n",
        "print(new_test_FNC[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_FNC: 49972, 49972\n",
            "test_FNC: 25413, 25413\n",
            "{'headline': \"Police officer in Charlie Hebdo investigation 'shot himself dead hours after the attack'\", 'body': 'Commissioner Helric Fredou, the person in charge of the Charlie Hebdo investigation, committed suicide on Thursday night in his office using his service weapon. The reasons behind the suicide are as yet unknown.\\n\\nAccording to an article on ‘Uprooted Palestinians’ blog:\\n\\nHe was deputy director of the regional police service since 2012. His father was a former police officer, his mother was a nurse in the emergency context CHU Limoges. He was single and had no children.\\n\\nAdvertisement\\n\\n\\n\\nAccording to the police union commissioner was depressed and experiencing burnout . In November 2013, the Commissioner Fredou had discovered the lifeless body of his colleague, number 3 of SRPJ Limoges, who had also committed suicide with his service weapon in his office. He was also 44 years old.\\n\\nThe Commissioner Fredou, like all agents SRPJ worked yesterday on the case of the massacre at the headquarters of Charlie Hebdo. In particular, he surveyed the family of one of the victims. He killed himself before completing its report. A psychological cell was set up in the police station.\\n\\nSource: https://uprootedpalestinians.wordpress.com/2015/01/09/another-mossad-victim-police-chief-helric-fredou-investigating-charlie-hebdo-commits-suicide/\\n\\nThis story has received very little coverage in the media as yet. Are the circumstances around his death suspicious? We will keep you updated with this story as it progresses.\\n\\n- See more at: http://yournewswire.com/police-chief-in-charge-of-paris-attacks-commits-suicide/#sthash.JzwB16lg.dpuf', 'stance': 3}\n",
            "{'headline': \"Police officer in Charlie Hebdo investigation 'shot himself dead hours after the attack'\", 'body': 'Commissioner Helric Fredou, the person in charge of the Charlie Hebdo investigation, committed suicide on Thursday night in his office using his service weapon. The reasons behind the suicide are as yet unknown.\\n\\nAccording to an article on ‘Uprooted Palestinians’ blog:\\n\\nHe was deputy director of the regional police service since 2012. His father was a former police officer, his mother was a nurse in the emergency context CHU Limoges. He was single and had no children.\\n\\nAdvertisement\\n\\n\\n\\nAccording to the police union commissioner was depressed and experiencing burnout . In November 2013, the Commissioner Fredou had discovered the lifeless body of his colleague, number 3 of SRPJ Limoges, who had also committed suicide with his service weapon in his office. He was also 44 years old.\\n\\nThe Commissioner Fredou, like all agents SRPJ worked yesterday on the case of the massacre at the headquarters of Charlie Hebdo. In particular, he surveyed the family of one of the victims. He killed himself before completing its report. A psychological cell was set up in the police station.\\n\\nSource: https://uprootedpalestinians.wordpress.com/2015/01/09/another-mossad-victim-police-chief-helric-fredou-investigating-charlie-hebdo-commits-suicide/\\n\\nThis story has received very little coverage in the media as yet. Are the circumstances around his death suspicious? We will keep you updated with this story as it progresses.\\n\\n- See more at: http://yournewswire.com/police-chief-in-charge-of-paris-attacks-commits-suicide/#sthash.JzwB16lg.dpuf', 'stance': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMT0MA5XWLp4"
      },
      "source": [
        "# Embedding layer\n",
        "Here we create the embedding layer using a pre-trained model either from `torchtext` or from `gensim`. We use the Field object from `torchtext` to build the vocabulary and store the vectors before creating the embedding layer using `torch.nn.Embedding`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-I5HeqmMvJj"
      },
      "source": [
        "## Create Field and vocabulariy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-YwAFdEdf7X",
        "outputId": "1b38aa79-7bc8-477d-9172-3f6a8ee290be"
      },
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "    Here we use tochtext to create the model.\n",
        "    We have to define the Field objects corresponding to text fields (headline\n",
        "    and body) and label fields (stance)\n",
        "    We also have to create a Dataset of Examples \n",
        "    See torchtext.legacy.data for more information\n",
        "\n",
        "    NOTE: This part takes time: I add an if to avoid execution when not needed\n",
        "\"\"\"\n",
        "\n",
        "if True:\n",
        "    CUSTOM_FILTERS = [lambda x: x.lower(), lambda x: re.sub('\\W+',' ',x) ,\\\n",
        "                        prep.strip_tags, prep.strip_punctuation, \\\n",
        "                        prep.strip_punctuation2, prep.strip_multiple_whitespaces, \\\n",
        "                        prep.strip_numeric, prep.remove_stopwords, prep.strip_short]\n",
        "    processing = lambda x: prep.preprocess_string(x, CUSTOM_FILTERS)\n",
        "  # use torchtext to define the dataset field containing text\n",
        "    TEXT = Field(sequential=True, init_token='<START>', \\\n",
        "                              eos_token='<END>',lower=True, tokenize = processing,\\\n",
        "                              batch_first=True,\\\n",
        "                              is_target=False)\n",
        "    LABEL = Field(sequential=False, use_vocab=False,\\\n",
        "                               batch_first=True,\\\n",
        "                                is_target=True)\n",
        "\n",
        "#TRAIN\n",
        "    fields = {'headline': ('headline', TEXT),\\\n",
        "            'body': ('body', TEXT), 'stance':('stance', LABEL)}\n",
        "    examples = [Example.fromdict(data=row,fields=fields) \\\n",
        "              for row in new_data_FNC]\n",
        "\n",
        "    fields = {'headline': TEXT,\\\n",
        "                'body':  TEXT, 'stance':LABEL}\n",
        "  # load your dataset using torchtext\n",
        "    train_set = Dataset(examples=examples,fields=fields)\n",
        "\n",
        "#TEST\n",
        "    fields = {'headline': ('headline', TEXT),\\\n",
        "                'body': ('body', TEXT), 'stance':('stance', LABEL)}\n",
        "    examples = [Example.fromdict(data=row,fields=fields) \\\n",
        "                for row in new_test_FNC]\n",
        "\n",
        "    fields = {'headline': TEXT,\\\n",
        "                'body':  TEXT, 'stance':LABEL}\n",
        "    # load your dataset using torchtext\n",
        "    test_set = Dataset(examples=examples,fields=fields)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 17s, sys: 761 ms, total: 1min 18s\n",
            "Wall time: 1min 18s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KJE1RTXnur_"
      },
      "source": [
        "## Load vectors into Field\n",
        "Here we create the embedding layer from pretrained word embeddings\n",
        "\n",
        "There are two kinds of pre trained word embeddings:\n",
        "  - The `gensim` pre trained models like Google News or FastText, or even the ones we trained\n",
        "  - The `torchtext` pretrained models\n",
        "\n",
        "Choose the option (`torchtext` or `gensim`) in the **Variable definition cell**.\n",
        "\n",
        "Then select the pretrained model to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjLwY3adAaBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a732043-b361-4e05-9a2d-acebc0cd74d3"
      },
      "source": [
        "%%time\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "if model_source == 'torchtext':\n",
        "    '''\n",
        "    Option 1: Use a torchtext pretrained model\n",
        "    See https://torchtext.readthedocs.io/en/latest/vocab.html#pretrained-aliases\n",
        "    '''\n",
        "    # Can take several minutes to load\n",
        "    try:\n",
        "        TEXT.build_vocab(train_set, test_set, vectors = pretrained_model)\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "    \n",
        "elif model_source == 'gensim':\n",
        "    \"\"\"\n",
        "    Option 2: Load a gensim Word2Vec model and use it to feed the Field\n",
        "    Gensim models are sometimes very heavy, so instead of using the gensim \n",
        "    download api, it is better to use directly wget when possible\n",
        "    Make sure the downloaded file can be opened with \n",
        "        gensim.models.KeyedVectors.load_word2vec_format\n",
        "    This means that we do not load models, but only vectors.\n",
        "    From a gensim model, they can be saved using \n",
        "        model.wv.save_word2vec_format('data/embeddings/word2vec_wv')\n",
        "    \"\"\"        \n",
        "    cwd = os.getcwd()\n",
        "    %cd /content\n",
        "    !mkdir downloaded_embeddings\n",
        "    %cd /content/downloaded_embeddings\n",
        "\n",
        "    if pretrained_model == \"gensim_google_news\":\n",
        "        !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "        path_model = \"/content/downloaded_embeddings/GoogleNews-vectors-negative300.bin.gz\"\n",
        "        binary = True\n",
        "\n",
        "    elif pretrained_model == \"gensim_fastText\":\n",
        "        !wget -c \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip\"\n",
        "        !unzip \"wiki-news-300d-1M-subword.vec.zip\"\n",
        "        path_model = \"/content/wiki-news-300d-1M-subword.vec\"\n",
        "        binary = False\n",
        "    else:\n",
        "        raise Exception(f\"For the moment the behaviour for the gensim model {pretrained_model} is not defined. Define it yourself and make sure the embedding layer is created in the end\")\n",
        "\n",
        "    %cd $cwd   \n",
        "\n",
        "\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(path_model, binary=True)\n",
        "    TEXT.build_vocab(train_set, test_set)\n",
        "    W2V_SIZE = model.vector_size\n",
        "    word2vec_vectors = []    \n",
        "    for token, idx in TEXT.vocab.stoi.items():\n",
        "        if token in model.key_to_index:\n",
        "            word2vec_vectors.append(torch.FloatTensor(model[token]))\n",
        "        else:\n",
        "            word2vec_vectors.append(torch.zeros(W2V_SIZE))\n",
        "\n",
        "    TEXT.vocab.set_vectors(TEXT.vocab.stoi, word2vec_vectors, W2V_SIZE)\n",
        "else:\n",
        "    print(\"Please choose a valid combination of model_source and pretrained_model\")\n",
        "    print(\"You can also use your own. Just make sure the embedding layer is created in the end\")\n",
        "    \n",
        "\n",
        "# Finally, we use the TEXT Field to build de embedding layer\n",
        "embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(TEXT.vocab.vectors))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/jmt_pre-trained_embeddings.tar.gz: 956MB [00:31, 30.7MB/s]                           \n",
            "100%|█████████▉| 873800/874474 [00:32<00:00, 27249.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 1s, sys: 5.99 s, total: 1min 7s\n",
            "Wall time: 1min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMCOpTvtoUpm"
      },
      "source": [
        "### Lets see some embeddings (and how to get them from the torchtext Field)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVueZQVcoX77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b6f844-8cee-4d21-c9af-28a612187222"
      },
      "source": [
        "word = \"france\"\n",
        "idx = TEXT.vocab.stoi[word]\n",
        "print(TEXT.vocab.vectors[idx])\n",
        "print(embedding(torch.LongTensor([idx])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.1108,  0.0496, -0.3659,  0.1553,  0.2015, -0.3966,  0.4557, -0.0998,\n",
            "        -0.2571,  0.5547, -0.0774, -0.5690, -0.1219, -0.1533, -0.0466,  0.2399,\n",
            "        -0.1444, -0.2318, -0.0998, -0.2557,  0.1196, -0.1827, -0.0118,  0.1066,\n",
            "         0.2834, -0.1194, -0.0215, -0.6559,  0.0250,  0.2567,  0.0989,  0.0600,\n",
            "         0.0240,  0.5639,  0.3945, -0.1996,  0.7262,  0.1010, -0.0169,  0.4765,\n",
            "        -0.1962, -0.3235, -0.2615,  0.1709, -0.0277,  0.1526,  0.0438,  0.3332,\n",
            "         0.5722,  0.0711,  0.2582,  0.3299,  0.3401,  0.0364, -0.2402,  0.3414,\n",
            "         0.6255,  0.2837, -0.0239, -0.0128, -0.1895,  0.0613, -0.6104, -0.6515,\n",
            "        -0.0318,  0.3879, -0.1221,  0.1695, -0.1299,  0.1020,  0.1925, -0.2992,\n",
            "         0.1606,  0.0995,  0.2794,  0.3212,  0.0864, -0.1271, -0.0118,  0.3479,\n",
            "        -0.1919,  0.3591,  0.1044,  0.0555,  0.5122,  0.5846,  0.2741, -0.0792,\n",
            "         0.0687, -0.1109,  0.1828,  0.3940, -0.1043,  0.8642,  0.0877,  0.3487,\n",
            "        -0.6196, -0.1305,  0.2653,  0.4306])\n",
            "tensor([[-0.1108,  0.0496, -0.3659,  0.1553,  0.2015, -0.3966,  0.4557, -0.0998,\n",
            "         -0.2571,  0.5547, -0.0774, -0.5690, -0.1219, -0.1533, -0.0466,  0.2399,\n",
            "         -0.1444, -0.2318, -0.0998, -0.2557,  0.1196, -0.1827, -0.0118,  0.1066,\n",
            "          0.2834, -0.1194, -0.0215, -0.6559,  0.0250,  0.2567,  0.0989,  0.0600,\n",
            "          0.0240,  0.5639,  0.3945, -0.1996,  0.7262,  0.1010, -0.0169,  0.4765,\n",
            "         -0.1962, -0.3235, -0.2615,  0.1709, -0.0277,  0.1526,  0.0438,  0.3332,\n",
            "          0.5722,  0.0711,  0.2582,  0.3299,  0.3401,  0.0364, -0.2402,  0.3414,\n",
            "          0.6255,  0.2837, -0.0239, -0.0128, -0.1895,  0.0613, -0.6104, -0.6515,\n",
            "         -0.0318,  0.3879, -0.1221,  0.1695, -0.1299,  0.1020,  0.1925, -0.2992,\n",
            "          0.1606,  0.0995,  0.2794,  0.3212,  0.0864, -0.1271, -0.0118,  0.3479,\n",
            "         -0.1919,  0.3591,  0.1044,  0.0555,  0.5122,  0.5846,  0.2741, -0.0792,\n",
            "          0.0687, -0.1109,  0.1828,  0.3940, -0.1043,  0.8642,  0.0877,  0.3487,\n",
            "         -0.6196, -0.1305,  0.2653,  0.4306]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AlSwjSpdUJH"
      },
      "source": [
        "### Check for missing words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4_PeCd-33ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bd315d-8863-44c2-dc3d-5f205e06c4be"
      },
      "source": [
        "missing_words = []\n",
        "for i, w in enumerate(TEXT.vocab.itos):\n",
        "    cond = ((TEXT.vocab.vectors[i] == 0.0).sum().item()==embedding.embedding_dim)\n",
        "    if i>3 and cond: missing_words.append(w)\n",
        "\n",
        "N = len(missing_words)\n",
        "print(f'Total number of words: {len(TEXT.vocab)}')\n",
        "print(f'Num of missing words: {N}')\n",
        "ind = np.random.randint(N, size=min(7,N))\n",
        "print(f'Some of them: {[missing_words[i] for i in ind]}')\n",
        "\n",
        "# The vector associated to this words is the zero vector, the same used for <unk>, <pad>, <sos>, <eos>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words: 27863\n",
            "Num of missing words: 109\n",
            "Some of them: ['كبار', 'تبوك', 'القيادة', 'البياض', 'أقدام', 'الفائتة', 'يوسف']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK7ZKgTuWWdg"
      },
      "source": [
        "# Training the model: Preliminaries\n",
        "Here we create:\n",
        "  - A [*BucketIterator*](https://torchtext.readthedocs.io/en/latest/data.html#bucketiterator) to go through the data using `torchtext` in a way that suits the *LSTM* model \n",
        "  - The *LSTM* model we are going to use\n",
        "  - Helper functions to be used during the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsFPIdUpwF2U"
      },
      "source": [
        "## BucketIterator\n",
        "The bucket iterator uses the TEXT Field object we created implicitly because it uses the torchtext datasets we created with them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVXElqWx_pwY"
      },
      "source": [
        "# Using splits\n",
        "train_loader, test_loader = BucketIterator.splits((train_set, test_set), batch_sizes=(32,32), device=device,\\\n",
        "                                       sort_key = lambda x: len(x.body),\n",
        "                                       sort=False,\n",
        "                                       sort_within_batch=True,\n",
        "                                       shuffle=True,\n",
        "                                       repeat=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhVa5s2WpWzb"
      },
      "source": [
        "Here are some manipulations of the BucketIterator to show how it works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSOkXqqk2D5_",
        "outputId": "ad49799c-0d2e-41eb-ef2b-e5b04cb89cea"
      },
      "source": [
        "# Example of a batch using the .batches functions\n",
        "# The for loop has to be stoped manually, as the batches will be recalculated \n",
        "# when the iterator is exhausted\n",
        "train_loader.create_batches()\n",
        "for batch in train_loader.batches:\n",
        "    # batch is a list of elements of type Example\n",
        "    print(f'batch is of type {type(batch)}')\n",
        "    for x in batch:\n",
        "        # x is an Example\n",
        "        print('Headline')\n",
        "        print(x.headline)\n",
        "        print('Body')\n",
        "        print(x.body)\n",
        "        # NOTICE: No padding or numeralization is being done!\n",
        "        break\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch is of type <class 'list'>\n",
            "Headline\n",
            "['christian', 'bale', 'play', 'steve', 'jobs', 'new', 'movie']\n",
            "Body\n",
            "['sprawling', 'facebook', 'post', 'subsequent', 'interview', 'radar', 'online', 'woman', 'named', 'taylor', 'lianne', 'chandler', 'number', 'fascinating', 'claims', 'alleged', 'relationship', 'michael', 'phelps', 'born', 'intersex', 'ambiguous', 'genitalia', 'met', 'olympic', 'swimmer', 'tinder', 'fucked', 'said', 'olympic', 'swimmer', 'date', 'watching', 'football', 'wondered', 'freaked', 'relationship', 'michael', 'phelps', 'exposed', 'year', 'old', 'area', 'woman', 'begins', 'post', 'truth', 'born', 'intersex', 'named', 'david', 'roy', 'fitch', 'birth', 'time', 'walk', 'talk', 'clear', 'girl', 'dressed', 'early', 'teens', 'medically', 'diagnosed', 'went', 'testosterone', 'blockers', 'estrogen', 'enhancers', 'birth', 'certificate', 'modified', 'teenager', 'prior', 'corrective', 'surgery', 'citing', 'anonymous', 'tipster', 'chandler', 'hollywood', 'gossip', 'previously', 'named', 'chandler', 'michael', 'phelps', 'companion', 'night', 'latest', 'dui', 'claim', 'repeated', 'recent', 'facebook', 'post', 'yes', 'michael', 'night', 'dui', 'story', 'uber', 'crying', 'wanting', 'stay', 'night', 'true', 'lot', 'things', 'printed', 'lies', 'probably', 'going', 'lose', 'brand', 'team', 'wants', 'protect', 'things', 'went', 'rehab', 'came', 'attention', 'relationship', 'best', 'light', 'man', 'intimacy', 'felt', 'comfortable', 'woman', 'today', 'chandler', 'followed', 'announcement', 'giving', 'radar', 'online', 'additional', 'details', 'supposed', 'date', 'phelps', 'meeting', 'tinder', 'involved', 'watching', 'baltimore', 'ravens', 'game', 'athlete', 'home', 'thing', 'led', 'love', 'halftime', 'chandler', 'told', 'site', 'later', 'sex', 'chandler', 'says', 'told', 'phelps', 'past', 'threat', 'media', 'exposing', 'forced', 'telling', 'story', 'power', 'away', 'media', 'tabloids', 'want', 'hurt', 'michael', 'probably', 'lose', 'hurts', 'like', 'hell', 'image', 'facebook', 'daily', 'mail']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQWZDf_B2ENz",
        "outputId": "bd18efa6-d706-477d-e855-bd7a6a1fb289"
      },
      "source": [
        "train_loader.create_batches()\n",
        "for batch in train_loader:\n",
        "    # batch is of type torchtext.data.Batch, which is the good one!\n",
        "    print(f'batch is of type {type(batch)}')\n",
        "    print(batch)\n",
        "    print('Headline')\n",
        "    print(batch.headline[:3])\n",
        "    print('Number of <pad> in each headline')\n",
        "    print((batch.headline == TEXT.vocab.stoi['<pad>']).sum(axis = 1))\n",
        "    print('Body shape')\n",
        "    print(batch.body.shape)\n",
        "    print('Number of <pad> in each body')\n",
        "    print((batch.body == TEXT.vocab.stoi['<pad>']).sum(axis = 1))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch is of type <class 'torchtext.legacy.data.batch.Batch'>\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.headline]:[torch.cuda.LongTensor of size 32x18 (GPU 0)]\n",
            "\t[.body]:[torch.cuda.LongTensor of size 32x108 (GPU 0)]\n",
            "\t[.stance]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
            "Headline\n",
            "tensor([[   2, 4773,  120,  262,  407,  140, 5235,   36, 1315, 2617,  120,  748,\n",
            "          218,  282,    3,    1,    1,    1],\n",
            "        [   2,  324,  428,  299, 1233,   28,   29,    3,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1],\n",
            "        [   2, 1403,   45,  101,   61,   64,    3,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1]], device='cuda:0')\n",
            "Number of <pad> in each headline\n",
            "tensor([ 3, 10, 11,  9, 11, 10, 10,  0,  9,  8, 10,  5, 11, 11,  7,  8, 10,  5,\n",
            "         7,  8,  5,  6,  6,  7,  8,  5, 10, 11, 10,  6, 13, 10],\n",
            "       device='cuda:0')\n",
            "Body shape\n",
            "torch.Size([32, 108])\n",
            "Number of <pad> in each body\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IIeHKftIFIn",
        "outputId": "3cb30f16-4f91-43be-9e4b-ff2bb02ceffa"
      },
      "source": [
        "headline = ' '.join([TEXT.vocab.itos[i] for i in batch.headline[0,:]])\n",
        "print(headline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> purdon ebola hoax texas town quarantined family test positive ebola virus article fake <END> <pad> <pad> <pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPbDqvir6lS8"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlobJ16O6rX1"
      },
      "source": [
        "class LSTM_encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_layer, embedding_size, dimension=128, agg='last',\\\n",
        "                 headline_output_size=128,body_output_size=128, mlp_size=100, do_dropout=True):\n",
        "        super(LSTM_encoder, self).__init__()\n",
        "\n",
        "        self.embedding = embedding_layer\n",
        "        # we can load a preecomputed embedding and freeze it during training\n",
        "        # we only need to separately train the Word2Vec or ELMO and import the matrix here\n",
        "        self.dimension = dimension\n",
        "        self.lstm = nn.LSTM(input_size=embedding_size,\n",
        "                            hidden_size=dimension,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        \n",
        "        self.drop = nn.Dropout(p=0.4)\n",
        "\n",
        "        self.fc_headline = nn.Linear(2*dimension, headline_output_size) #because they concat last and first (bidir)\n",
        "        self.fc_body = nn.Linear(2*dimension, body_output_size) #because they concat last and first (bidir)\n",
        "        self.agg = agg\n",
        "\n",
        "        self.mlp_fc_1 = nn.Linear(headline_output_size + body_output_size, mlp_size)\n",
        "        self.mlp_fc_2 = nn.Linear(mlp_size, 4)\n",
        "        self.do_dropout = do_dropout\n",
        "\n",
        "    def forward(self, body, body_len, headline, headline_len):\n",
        "\n",
        "        body_emb = self.embedding(body)\n",
        "        headline_emb = self.embedding(headline)\n",
        "        #allows you to work with different sizes of inputs in the same tensor for batch calculation\n",
        "        \n",
        "        ### BODY\n",
        "        lengths = torch.zeros(body.shape[0]) + body.shape[1]\n",
        "        # print(\"body_emb: \", body_emb.shape)\n",
        "        packed = pack_padded_sequence(body_emb, lengths=lengths, enforce_sorted=False, batch_first=True)\n",
        "\n",
        "        packed_output, _ = self.lstm(packed)\n",
        "        # print(packed_output.shape)\n",
        "\n",
        "        output, _ = pad_packed_sequence(packed_output)        \n",
        "        #unpack the output (reverse operation of pack_padded_sequence)\n",
        "\n",
        "        # concat the last and first states of the bidir LSTM\n",
        "\n",
        "        # Choose output\n",
        "        # output has shape (seq_len, batch_size, num_directions * hidden_size)\n",
        "        # print(\"output: \", output.shape)\n",
        "\n",
        "        # Last\n",
        "        if self.agg=='last':\n",
        "            out_forward = output[-1, :, :self.dimension]\n",
        "            out_reverse = output[0, :, self.dimension:]   # We were taking the last (-1)\n",
        "        # Max\n",
        "        if self.agg=='max':\n",
        "            pass\n",
        "        # --------- Concat -------------\n",
        "        # print(\"out_forward: \", out_forward.shape)\n",
        "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "  \n",
        "        # print(\"out_reduced: \", out_reduced.shape)\n",
        "        if self.do_dropout:\n",
        "            x = self.drop(out_reduced)\n",
        "        else:\n",
        "            x = out_reduced\n",
        "        # print(\"dropout: \", x.shape)\n",
        "        text_fea = self.fc_body(x)  \n",
        "        # print(\"fc: \", x.shape)\n",
        "        # text_fea = torch.squeeze(x,1)   # Squeeze does not change anything when batch size > 1\n",
        "        # print(\"text_fea: \", text_fea.shape)\n",
        "        body_out = torch.sigmoid(text_fea) # activation\n",
        "\n",
        "\n",
        "        ### HEADLINE\n",
        "        lengths = torch.zeros(headline.shape[0]) + headline.shape[1]\n",
        "        packed = pack_padded_sequence(headline_emb, lengths=lengths, enforce_sorted=False, batch_first=True)\n",
        "        packed_output, _ = self.lstm(packed)\n",
        "        output, _ = pad_packed_sequence(packed_output)        \n",
        "        #unpack the output (reverse operation of pack_padded_sequence)\n",
        "\n",
        "        # concat the last and first states of the bidir LSTM\n",
        "\n",
        "        # Choose output\n",
        "        # output has shape (seq_len, batch_size, num_directions * hidden_size)\n",
        "\n",
        "        # Last\n",
        "        if self.agg=='last':\n",
        "            out_forward = output[-1, :, :self.dimension]\n",
        "            out_reverse = output[0, :, self.dimension:]\n",
        "        # Max\n",
        "        if self.agg=='max':\n",
        "            pass\n",
        "        # --------- Concat -------------\n",
        "        out_reduced = torch.cat((out_forward, out_reverse), 1) \n",
        "        if self.do_dropout:\n",
        "            x = self.drop(out_reduced)\n",
        "        else:\n",
        "            x = out_reduced\n",
        "        text_fea = self.fc_headline(x)\n",
        "        # text_fea = torch.squeeze(self.fc_headline(self.drop(out_reduced)),1)   \n",
        "        headline_out = torch.sigmoid(text_fea) # activation      \n",
        "\n",
        "        out = torch.cat((headline_out, body_out),1) \n",
        "        \n",
        "        ### MLP part\n",
        "        x = self.mlp_fc_1(out)\n",
        "        if self.do_dropout:\n",
        "            x = self.drop(x)\n",
        "        out = self.mlp_fc_2(x)        \n",
        "        # Extra dropout?\n",
        "        # out = self.drop(out)\n",
        "        return out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnLeim0DCa7F"
      },
      "source": [
        "## Checkpoints\n",
        "Functions used to save and load the state of the training and different results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1vXnc_rA3NP"
      },
      "source": [
        "# Save and Load Functions\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
        "    \"\"\" Saves the state_dict of the model, the optimizer and also the current\n",
        "        validation loss\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        save_path: str\n",
        "            Path where the data will be saved to.\n",
        "\n",
        "        model: pytorch model\n",
        "            Must have a state_dict() method\n",
        "        \n",
        "        optimizer: pytorch optimizer\n",
        "            Must have a state_dict() method\n",
        "\n",
        "        valid_loss: pytorch tensor\n",
        "            The current validation loss during training \n",
        "    \"\"\"\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model, optimizer):\n",
        "    \"\"\" Loads the model and optimizer state_dict\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        load_path: str\n",
        "            Path to the file\n",
        "\n",
        "        model: pytorch model\n",
        "            The loaded state_dict will be loaded into this model object\n",
        "\n",
        "        optimizer: pytorch optimizer\n",
        "            The loaded optimizer state_dict will be loaded to this optimizer object\n",
        "\n",
        "    \"\"\"\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    \n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "    \"\"\" Saves a list of tensors representing the evolution of the training\n",
        "        and validations losses.\n",
        "    \"\"\"\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "    \"\"\" Load the lists containing the evolution of the training and validation losses\n",
        "    \"\"\"\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n",
        "\n",
        "def categorize(output, labels, classes):\n",
        "    \"\"\" Given the output of the model, use the torch.argmax function to determine\n",
        "        the predicted class and compare it to labels.\n",
        "        For each class in classes, return the number of correct predictions for \n",
        "        this class\n",
        "    \"\"\"\n",
        "    pred = output.argmax(dim=1)\n",
        "    hits = [((pred==c)*(labels==c)).sum().item() for c in classes]\n",
        "    return np.array(hits)\n",
        "\n",
        "def FNC_metric (X,Y, unrelated_index = 0) :\n",
        "    \"\"\" Metric used during the Fake News Challenge\n",
        "        X and Y are arrays containing the predicted and real class\n",
        "    \"\"\"\n",
        "    lvl1 =  0.25 * ((X == unrelated_index ) == (Y == unrelated_index)).sum()\n",
        "    lvl2 = 0.75 * ((X != unrelated_index) & (X == Y)).sum()\n",
        "    return lvl1 + lvl2\n",
        "\n",
        "\n",
        "\n",
        "def metric_on_loader(model, loader, metric):\n",
        "    \"\"\" Calculate a cumulative metric on a loader by simply adding up the value\n",
        "        of the metric obtained on each batch\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        model: pytorch model compatible with `loader` and `metric`\n",
        "\n",
        "        loader: A torchtext data loader.\n",
        "            In our case, we use BucketIterator\n",
        "\n",
        "        metric: function\n",
        "            Takes two torch tensors and computes a scalar. \n",
        "            Both tensors received by the metric are class predictions\n",
        "\n",
        "    \"\"\"\n",
        "    model.eval()    \n",
        "    total_metric = 0\n",
        "    N = len(loader)\n",
        "    for i, batch in enumerate(loader): #I don't know where the size are coming from\n",
        "            \n",
        "        if i == N: break\n",
        "        labels = batch.stance\n",
        "        body = batch.body\n",
        "        headline = batch.headline\n",
        "        body_len = torch.Tensor(batch.body.shape[0])\n",
        "        headline_len = torch.Tensor(batch.headline.shape[0])\n",
        "        body = body.to(device)\n",
        "        body_len = body_len.to(device)\n",
        "        headline = headline.to(device)\n",
        "        headline_len = headline_len.to(device)\n",
        "        output = model(body, body_len, headline, headline_len) \n",
        "        total_metric += metric(labels.detach().cpu().numpy(),output.argmax(dim=1).detach().cpu().numpy())\n",
        "    \n",
        "\n",
        "    return total_metric\n",
        "\n",
        "\n",
        "\n",
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.CrossEntropyLoss(),\n",
        "          train_loader = train_loader,\n",
        "          valid_loader = test_loader, #to be changed once we split\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_loader),\n",
        "          file_path = 'FNC_data/LSTM_training',\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \"\"\" Trains the model using the specified optimizer and loss criterion. This\n",
        "        function also saves metrics and other information during execution\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model: pytorch model to train\n",
        "\n",
        "        optimizer: pytorch optimizer to use for training\n",
        "\n",
        "        criterion: pytorch loss\n",
        "\n",
        "        train_loader: pytorch Dataloader or torchtext Iterator\n",
        "            Used to feed batches of training data to the \n",
        "            \n",
        "        valid_loader: pytorch Dataloader or torchtext Iterator\n",
        "            Used to feed batches of validation data to the model during the\n",
        "            validation phase\n",
        "\n",
        "        num_epochs: int\n",
        "            Epochs of training\n",
        "\n",
        "        eval_every: Of the global steps (epochs x num_training_batches), \n",
        "            determines the number of steps of training before each validation phase\n",
        "\n",
        "        file_path: str\n",
        "            Path to the folder to store the metrics and model checkpoints that\n",
        "            will be generated through out the training\n",
        "\n",
        "        best_valid_loss: float\n",
        "            Best validation loss so far. Used to determine when to save the \n",
        "            model, as it is only saved when the best validation loss is improved\n",
        "        \n",
        "    \"\"\"\n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "    classes = [0,1,2,3]\n",
        "    hits_class_total = []\n",
        "    hits_class_total_train = []\n",
        "    conf_batch_train = np.zeros((len(classes),len(classes)))\n",
        "    conf_batch_test = np.zeros((len(classes),len(classes)))\n",
        "    conf_list_train = []\n",
        "    conf_list_test = []\n",
        "    total_FNC_metric = []\n",
        "\n",
        "    N_train = len(train_loader)\n",
        "    N_test = len(valid_loader)\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        hits_class_train = np.array([0 for i in classes])\n",
        "        conf_batch_train = np.zeros((len(classes),len(classes)))\n",
        "    \n",
        "        # print('Epoch [{}/{}]'.format(epoch+1, num_epochs)) \n",
        "        train_loader.create_batches()\n",
        "        for i, batch in enumerate(train_loader): #I don't know where the size are coming from\n",
        "            \n",
        "            '''\n",
        "            TRAIN LOOP\n",
        "            '''\n",
        "            if i == N_train: break\n",
        "            labels = batch.stance\n",
        "            body = batch.body\n",
        "            headline = batch.headline\n",
        "            body_len = torch.Tensor(batch.body.shape[0])\n",
        "            headline_len = torch.Tensor(batch.headline.shape[0])\n",
        "            body = body.to(device)\n",
        "            body_len = body_len.to(device)\n",
        "            headline = headline.to(device)\n",
        "            headline_len = headline_len.to(device)\n",
        "            output = model(body, body_len, headline, headline_len) # change inputs            \n",
        "            \n",
        "            \n",
        "            \n",
        "            loss = criterion(output, labels)\n",
        "            cats = categorize(output, labels, classes)\n",
        "            conf_batch_train += confusion_matrix(labels.detach().cpu().numpy(), output.argmax(dim=1).detach().cpu().numpy(), labels=classes)\n",
        "            hits_class_train += cats\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            # print('loss.backward()')\n",
        "            loss.backward()\n",
        "            # print('optimizer.step()')\n",
        "            optimizer.step()\n",
        "            # print('done')  \n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            # print(loss.item())\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            '''\n",
        "            EVALUATION\n",
        "            '''\n",
        "            if global_step % eval_every == 0:\n",
        "                \n",
        "                hits_class = np.array([0 for i in classes])\n",
        "                conf_batch_test = np.zeros((len(classes),len(classes)))\n",
        "                \n",
        "                # print(\"eval\")\n",
        "                model.eval()\n",
        "                FNC_met = 0\n",
        "                with torch.no_grad():                    \n",
        "                  # validation loop\n",
        "                  valid_loader.create_batches()\n",
        "                  for j, batch_t in enumerate(valid_loader):\n",
        "                      if j == N_test: break\n",
        "                      labels = batch_t.stance\n",
        "                      body = batch_t.body\n",
        "                      headline = batch_t.headline\n",
        "                      body_len = torch.Tensor(batch_t.body.shape[0])\n",
        "                      headline_len = torch.Tensor(batch_t.headline.shape[0])\n",
        "                      body = body.to(device)\n",
        "                      body_len = body_len.to(device)\n",
        "                      headline = headline.to(device)\n",
        "                      headline_len = headline_len.to(device)\n",
        "                      output = model(body, body_len, headline, headline_len) # change inputs   \n",
        "                      # print(\"criterion(output, labels)\")\n",
        "                      loss = criterion(output, labels)\n",
        "                      cats = categorize(output, labels, classes)\n",
        "                      hits_class += cats\n",
        "                      conf_batch_train += confusion_matrix(labels.detach().cpu().numpy(), output.argmax(dim=1).detach().cpu().numpy(), labels=classes)\n",
        "                      valid_running_loss += loss.item()\n",
        "                      FNC_met += FNC_metric(labels.detach().cpu().numpy(),output.argmax(dim=1).detach().cpu().numpy())\n",
        "\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(test_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                hits_class_total.append(hits_class)\n",
        "                hits_class_total_train.append(hits_class_train)\n",
        "\n",
        "                conf_list_test.append(conf_batch_test)\n",
        "                conf_list_train.append(conf_batch_train)\n",
        "                total_FNC_metric.append(FNC_met)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                # print(hits_class_total)\n",
        "                torch.save({'accuracy_class': hits_class_total, 'accuracy_class_train': hits_class_total_train}, file_path + '/accuracy_per_class.pt')\n",
        "                torch.save({'conf_train': conf_list_train, 'conf_test': conf_list_test}, file_path + '/conf_matrices.pt')\n",
        "                torch.save({'FNC_metric': total_FNC_metric}, file_path + '/FNC_metric.pt')\n",
        "                save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
        "                    \n",
        "      \n",
        "        \n",
        "\n",
        "    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7uJxsHoh0dw"
      },
      "source": [
        "# Training the model: Launch training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qOZk5AAnB15"
      },
      "source": [
        "## Launch training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjY7_rwD0AV4"
      },
      "source": [
        "model = LSTM_encoder(embedding, embedding.embedding_dim, dimension=128, agg='last',\\\n",
        "                     headline_output_size=200,\\\n",
        "                     body_output_size=200, mlp_size=128, do_dropout=True)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "best_valid_loss = float(\"Inf\")\n",
        "\n",
        "destination_folder = 'data/LSTM_training'\n",
        "\n",
        "save_checkpoint(destination_folder + '/model_start.pt', model, optimizer, best_valid_loss)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(model=model, optimizer=optimizer, criterion = criterion, num_epochs=40, train_loader=train_loader, \\\n",
        "      valid_loader=test_loader, file_path = destination_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1POoD6kUnFZJ"
      },
      "source": [
        "## Download the results\n",
        "Run this cell and wait for the pop up window to appear. Choose a destination for the download\n",
        "\n",
        "It may take a while\n",
        "\n",
        "If you want to do it manually, use the Colab file explorer in the left side, go to the desired folder/file and download it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GFFLGtOw4wKJ",
        "outputId": "b7e2654d-feaf-44f0-eec9-75d96f8bade6"
      },
      "source": [
        "from google.colab import files\n",
        "!pwd\n",
        "to_zip = destination_folder\n",
        "!zip -r data/to_download.zip $to_zip\n",
        "files.download(\"data/to_download.zip\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_daf6bc7a-12f7-4be9-8e81-780d34484ba0\", \"to_download.zip\", 26399139)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nc0lyLKWzm4"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVZEMnvDLY2p"
      },
      "source": [
        "## FNC Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbO3fs_kLbVN"
      },
      "source": [
        "plot_FNC_metric = torch.load(destination_folder + '/FNC_metric.pt')['FNC_metric']\n",
        "plt.plot(plot_FNC_metric)\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('FNC_metric on the test set')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xK3phy1m69B"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdLSv36p2YVV"
      },
      "source": [
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoEkOn4UUcIk"
      },
      "source": [
        "## Accuracy per class: Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLs2c0JlUlS-"
      },
      "source": [
        "accu_c = torch.load(destination_folder+'/accuracy_per_class.pt', map_location='cpu')\n",
        "res = accu_c['accuracy_class_train']\n",
        "total = [0,0,0,0]\n",
        "N_max = len(train_loader)\n",
        "for i, b in enumerate(train_loader):\n",
        "  if i == N_max: break\n",
        "  x = b.stance.detach().to('cpu').numpy()\n",
        "  for c in range(4):\n",
        "    total[c] += (x==c).sum()\n",
        "\n",
        "to_plot = np.array(res) / np.array(total)\n",
        "to_plot = to_plot.T\n",
        "\n",
        "labels_dict = {'unrelated':0, 'agree':1, 'disagree':2, 'discuss':3}\n",
        "labels_names = {0:'unrelated', 1:'agree', 2:'disagree', 3:'discuss'}\n",
        "names = [labels_names[i] for i in range(4)]\n",
        "\n",
        "for i, n in enumerate(names):\n",
        "  plt.plot(to_plot[i,:], label=n)\n",
        "\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy per class: Train')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxBrdbFJUgO3"
      },
      "source": [
        "## Accuracy per class: Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-17Hk0bLOOLC"
      },
      "source": [
        "res = accu_c['accuracy_class']\n",
        "total = [0,0,0,0]\n",
        "N_max = len(test_loader)\n",
        "for i, b in enumerate(test_loader):\n",
        "  if i == N_max: break\n",
        "  x = b.stance.detach().to('cpu').numpy()\n",
        "  for c in range(4):\n",
        "    total[c] += (x==c).sum()\n",
        "\n",
        "to_plot = np.array(res) / np.array(total)\n",
        "to_plot = to_plot.T\n",
        "\n",
        "for i, n in enumerate(names):\n",
        "  plt.plot(to_plot[i,:], label=n)\n",
        "\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy per class: Validation')\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MRIyzRum_va"
      },
      "source": [
        "## Confusion matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-caoeBJSdOP"
      },
      "source": [
        "conf_matrices = torch.load(destination_folder+'/conf_matrices.pt', map_location='cpu')\n",
        "train_conf = conf_matrices['conf_train']\n",
        "\n",
        "# Select coordinates of the matrix to plot over the training\n",
        "# format is (True, Predicted)\n",
        "show_accuracy = [('unrelated', 'unrelated'), ('agree','agree'), ('disagree','disagree'), ('discuss','discuss')]\n",
        "divide_by = 'none'\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "labels_dict = {'unrelated':0, 'agree':1, 'disagree':2, 'discuss':3}\n",
        "for t,p in show_accuracy:\n",
        "  x, y =  labels_dict[t], labels_dict[p]\n",
        "  \n",
        "  # Define numerator\n",
        "  num = lambda M, x, y: M[x,y]\n",
        "\n",
        "  # Define denominator\n",
        "  den = lambda M, x, y: 1.0\n",
        "  if divide_by == 'row':\n",
        "    den = lambda M, x, y: M[x,:].sum()\n",
        "  elif divide_by == 'col':\n",
        "    den = lambda M, x, y: M[:,y].sum()\n",
        "\n",
        "  z = [num(C, x, y)/den(C,x,y) for C in train_conf]\n",
        "  plt.plot(z, label = '-'.join([t[:4], p[:4]]))\n",
        "\n",
        "plt.xlabel('Global Steps')\n",
        "# plt.ylabel('Accuracy on each class')\n",
        "plt.legend()\n",
        "plt.title('Evolution of confusion matrix')\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}